{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Bidirectional, Dense, Dropout, Activation, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "embedding_size = 100\n",
    "dropout=0.5\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train.Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vec = Word2Vec.load('./word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878856"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(train.Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(train_seq, maxlen=max_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(train.Lable).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embedding_size), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_count = 0\n",
    "unknown_freq = {}\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index >= vocab_size: \n",
    "            continue\n",
    "    try:\n",
    "        embedding_matrix[index, :] = word_vec.wv[word]\n",
    "    except KeyError:\n",
    "        unknown_freq[word] = tokenizer.word_counts[word]\n",
    "        unknown_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computation_graph():\n",
    "    model  = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, weights=[embedding_matrix], name='Embedding_Layer'))\n",
    "    model.add(Bidirectional(LSTM(units=100)))\n",
    "#     model.add(Dense(units=hyperparam['filters'], name='Dense_'+str(hyperparam['filters'])))\n",
    "    model.add(Dropout(rate=dropout, name = 'Dropout_' + str(dropout)))\n",
    "#     model.add(Activation(hyperparam['dense_activation'], name='Activation_'+str(hyperparam['dense_activation'])))\n",
    "    model.add(Dense(units=y_train.shape[1], activation='softmax', name='Dense_'+str(y_train.shape[1])+'_Softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = computation_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding_Layer (Embedding)  (None, None, 100)         87885600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "Dropout_0.5 (Dropout)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "Dense_14_Softmax (Dense)     (None, 14)                2814      \n",
      "=================================================================\n",
      "Total params: 88,049,214\n",
      "Trainable params: 88,049,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 566s 13ms/step - loss: 1.1764 - acc: 0.6340 - val_loss: 0.7851 - val_acc: 0.7598\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 559s 12ms/step - loss: 0.9249 - acc: 0.7173 - val_loss: 0.8413 - val_acc: 0.7610\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 519s 12ms/step - loss: 0.7333 - acc: 0.7890 - val_loss: 0.6841 - val_acc: 0.7954\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 516s 11ms/step - loss: 0.5941 - acc: 0.8278 - val_loss: 0.6429 - val_acc: 0.8124\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 527s 12ms/step - loss: 0.4815 - acc: 0.8643 - val_loss: 0.6093 - val_acc: 0.8198\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 518s 12ms/step - loss: 0.3912 - acc: 0.8910 - val_loss: 0.6143 - val_acc: 0.8190\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 520s 12ms/step - loss: 0.3112 - acc: 0.9153 - val_loss: 0.6263 - val_acc: 0.8280\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 520s 12ms/step - loss: 0.2506 - acc: 0.9320 - val_loss: 0.6908 - val_acc: 0.8172\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 541s 12ms/step - loss: 0.1970 - acc: 0.9478 - val_loss: 0.7025 - val_acc: 0.8100\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 549s 12ms/step - loss: 0.1380 - acc: 0.9655 - val_loss: 0.7557 - val_acc: 0.8032\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, y=y_train,\n",
    "                          validation_split = validation_split,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
